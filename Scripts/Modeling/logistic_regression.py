# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyiYur3lK0EbhcykO4yfZLwIcDxF9a8o
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings

warnings.filterwarnings('ignore')

test = pd.read_csv("TestSet.csv")
train = pd.read_csv("TrainSet.csv")

test_names=test["Name"]
train_names=train["Name"]

test=test.drop(["Name","Year","Pos","GP"],axis=1) #Drops categorical variables
train=train.drop(["Name","Year","Pos","GP"],axis=1)

train_X = train.drop(['Suc'], axis=1)
train_Y = train['Suc']
test_X = test.drop(["Suc"],axis=1)
test_Y = test["Suc"]

from sklearn.linear_model import LogisticRegression


# instantiate the model
logreg = LogisticRegression(solver='liblinear', random_state=0)


# fit the model
logreg.fit(train_X, train_Y)

y_pred_test = logreg.predict(test_X)

from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(test_Y, y_pred_test)))

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_Y, y_pred_test)

print('Confusion matrix\n\n', cm)

print('\nTrue Positives(TP) = ', cm[0,0])

print('\nTrue Negatives(TN) = ', cm[1,1])

print('\nFalse Positives(FP) = ', cm[0,1])

print('\nFalse Negatives(FN) = ', cm[1,0])